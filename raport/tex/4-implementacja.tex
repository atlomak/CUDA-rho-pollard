\newpage
\section{Implementacja}
Ta część pracy jest poświęcona implementacji całego systemu,
wraz ze szczegółowym opisem narzędzi oraz sprzętu wykorzystanego
przy pracy nad projektem.

\subsection{Oprogramowanie oraz platforma sprzętowa}
\subsubsection{Platforma sprzętowa}
\begin{itemize}
    \item OS: openSUSE Tumbleweed 20240812
    \item CPU: Intel Core i5-10600KF 4.10 GHz
    \item RAM: 16 GB
    \item GPU: GeForce RTX 2070 Super
\end{itemize}

\subsubsection{Program po stronie CPU}
Implementacja programu serwera działającego po stronie CPU, została wykonana z wykorzystaniem
języka Python w wersji 3.11.1 wraz z pakietem obliczeniowym SageMath w wersji 10.4.beta3.
Program klienta został zaimplementowany z wykorzystaniem języka Python w wersji 3.11.1 oraz
języka CUDA C++ w wersji 12.4.

\subsubsection{System budowania oraz kompilacja CUDA C++}
W celu zarządzania kompilacją części projektu napisanej w języku CUDA oraz C++,
zostało wykorzystane narzędzie \textit{CMake}.
\textit{CMake} zapewnia natywne wsparcie dla języka CUDA,
co znacznie upraszcza wszelkie zarządzanie kompilacją oraz linkowanie.
Dodatkowo, wymaga to znacznie mniej wstępnej konfiguracji niż analogiczne
rozwiązanie z wykorzystaniem samego narzędzia do budowania takiego jak \textit{Make} lub \textit{Ninja}.
\par
Program napisany w CUDA kompilowany był z wykorzystaniem kompilatora NVCC dostarczonego wraz z pakietem
\textit{CUDA Toolkit} w wersji 12.4.
NVCC do kompilacji kodu po stronie host'a (CPU) wykorzystuje kompilator z GCC w wersji 13.3.1.

\subsubsection{Testy}
Wszelkie testy poprawności implementacji rozwiązania są przeprowadzane z wykorzystaniem framework'a PyTest dla języka Python.
Python wraz z wykorzystaniem pakietu obliczeniowego SageMath generuje testowe dane, które
są przekazywane do programu działającego na GPU. Następnie, otrzymane wyniki są porównywane z rezultatami
obliczonymi przy wykorzystaniu SageMath. Wykorzystanie PyTest oraz SageMath znacznie przyśpieszyło pracę
nad implementacją operacji na krzywej eliptycznej oraz docelowej implementacji algorytmu Rho Pollard'a.
Możliwość szybkiego generowania dużych zbiorów danych testowych, pozwoliło na wczesne zauważenie wielu subtelnych błędów
na etapie implementacji.

\subsection{Program klienta Python}
Część podprogramu klienta napisana w języku Python, została zaimplementowana
za pomocą biblioteki \textit{Threading} z biblioteki standardowej.
Na początku działania programu, podprogram serwera uruchamia w kilku wątkach
funkcję \textit{GPUWorker} która stanowi implementację klienta.
Odpowiada ona za
dostosowywanie danych wejściowych do programu napisanego w CUDA, oraz
zwracanie wyników z powrotem do serwera. Sygnatura funkcji GPUWorker:
\begin{lstlisting}[language=Python]
def GPUworker(
    starting_params: StartingParameters,
    task_queue: Queue,
    result_queue: Queue, stream):
\end{lstlisting}

W celu komunikacji z serwerem, wykorzystywane są obiekty kolejek
\textit{queue.Queue} z biblioteki standardowej.
Ponieważ Python w wersji 3.12 nie wspiera równoległego wykonywania
się wątków \cite{GIL}, to zmiana wątku następuje po wywołaniu programu CUDA.
Następuje wtedy wywołanie systemowe \textit{Sleep} i wątek pozostaje nieaktywny
aż do zakończenie obliczeń przez GPU.
Dzięki kompilacji programu CUDA z flagą \textit{--default-stream per-thread},
każdy z wątków klienta uruchamia program na GPU w innym strumieniu CUDA. Pozwala
to na współbieżne wykonywanie się obliczeń na GPU uruchomionych przez każdy wątek, zarządzane przez
\textit{CUDA Scheduler}. \\
Implementacja struktur danych przekazywanych do programu CUDA:
\begin{lstlisting}[language=Python]
LIMBS = 5
class bn(ctypes.Structure):
    _fields_ = [("array", ctypes.c_uint32 * LIMBS)]

class small_bn(ctypes.Structure):
    _fields_ = [("array", ctypes.c_uint32 * ceil(LIMBS / 2))]
class EC_point(ctypes.Structure):
    _fields_ = [
        ("x", bn),
        ("y", bn),
        ("seed", bn),
        ("is_distinguish", ctypes.c_uint32),
    ]
\end{lstlisting}

\subsection{Program klienta CUDA}

Program uruchamiany na GPU został skompilowany jako dynamicznie linkowana
biblioteka (\textit{SHARED}). Aby zapewnić zgodność z interfejsem binarnym
(ABI) wykorzystywanym przez bibliotekę Ctypes, do głównej funkcji dodano
słowo kluczowe \texttt{extern "C"} (wydruk \ref{code:cuda-entry}).
Pozwoliło to uzyskać punkt wejścia zgodny
z konwencją wywołania C (C ABI). Główna funkcja, po otrzymaniu parametrów
startowych, alokuje odpowiednią pamięć CUDA oraz konfiguruje niezbędne
parametry uruchamiana dla głównego kernel'a, który realizuje algorytm
Rho Pollard'a. Po zakończeniu obliczeń, pamięć jest zwalniana, a wyniki
zapisywane w tablicach przekazanych jako dane wejściowe. W rezultacie,
tablica punktów startowych przekazana do programu pełni jednocześnie
funkcję miejsca zapisu wyników obliczeń.
\begin{lstlisting}[language=C++, caption={Prototyp funkcji wejściowej programu CUDA}, label=code:cuda-entry]
extern "C" {
void run_rho_pollard(
    EC_point *startingPts,
    uint32_t instances,
    uint32_t n, PCMP_point *precomputed_points,
    EC_parameters *parameters,
    int stream
    )
\end{lstlisting}
Następne podrozdziały są poświęcone implementacji poszczególnych
elementów składających się na kod głównego kernel'a CUDA.


\subsection{Arytmetyka na ciele}
\subsubsection{Reprezentacja długich liczb}
W przypadku ciała $F_{p}$, gdzie $p$ jest liczbą pierwszą o rozmiarze 79 bit, potrzebny jest co najmniej 79 bitowy typ danych, do samego przechowywania liczb.
Jednak nawet 79 bitowy typ danych nie wystarczy, aby przeprowadzić operację mnożenia
dwóch liczb 79 bitowych. W takim przypadku, wynik pośredni może być maksymalnie $2*79 = 158$ bitowy.
Dodatkowo, w celu reprezentacji takiej liczby, nie możemy się posłużyć wektorem składającym się z największego
dostępnego natywnie typu danych,
ponieważ wyniki pośrednie z operacji mnożenia lub dodawania mogą przekroczyć rozmiar słowa bitowego.
W tym celu musimy wykorzystać typ danych mniejszy od maksymalnego. W przypadku CUDA C++, największy wspierany typ danych
wynosi 64 bit, więc w celu reprezentacji liczb, w pracy wykorzystano wektor składający się z 32 bitowych słów.
Najbliższa wielokrotność liczby 32 bitowej, większa niż 158 bit to 160 bit,
dlatego w celu reprezentacji liczb na ciele, wykorzystywany jest wektor postaci:
$$
    \sum_{i = 0}^{4} x_i\cdot 2^{32}
$$
\begin{minipage}{\linewidth}
    Zaimplementowany jako tablica typu u\_int32 (wydruk \ref{code:bignum})
    \begin{lstlisting}[language=C++, caption={Struktura do przechowywanie długich liczb}, label=code:bignum]
struct bn
{
    uint32_t array[5];
};
\end{lstlisting}
\end{minipage}
Dla liczb, na których bezpośrednio nie będą wykonywane operacje arytmetyczne, wykorzystywany jest mniejszy, tymczasowy typ
danych.
Ograniczone zasoby szybkiej pamięci współdzielonej na GPU, wymuszają oszczędne zarządzanie pamięcią. W celu
przechowywania wstępnie obliczonych punktów w pamięci współdzielonej, wykorzystywany jest mniejszy typ danych
(wydruk \ref{code:small-bn})
\begin{lstlisting}[language=C++, caption={Mniejszy typ danych do długich liczb}, label=code:small-bn]
struct small_bn
{
    uint32_t array[3];
};
\end{lstlisting}
\par
Liczby w takiej postaci, przed przeprowadzeniem na nich operacji arytmetycznych, są ładowane z pamięci współdzielonej i z powrotem konwertowane na większy typ danych.
Pozwala to zaoszczędzić $ 2 \cdot 32$ bitów na każdej liczbie, co w przypadku punktu składającego się z dwóch współrzędnych
daje oszczędność $2 \cdot 2 \cdot 32 = 128$ bitów na punkt znajdujący się w pamięci.

\subsubsection{Operacje na długich liczbach}
Do operacji na długich liczbach, została odpowiednio dostosowana mała biblioteka
dostępną w domenie publicznej: \textit{tiny-bignum-c}.
Biblioteka ta dostarcza podstawowe operacje na dużych liczbach w postaci wektorów, takie jak dodawanie, odejmowanie, mnożenie czy dzielenie.
Wykorzystuje w tym celu standardowe algorytmy \cite{Menezes2001} wykorzystywane przy obliczeniach \textit{multiple precision}.
Dużą zaletą ten biblioteki jest jej prostota i brak wykorzystania standardowej biblioteki C oraz dynamicznej alokacji pamięci.
Wszystkie operacja są wykonywane z wykorzystaniem stosu,
co pozwala na jej wykorzystanie w środowiskach takich jak GPU, gdzie dostęp do dynamicznej alokacji pamieci jest ograniczony.
W większości przypadków, aby dostosować kod z biblioteki do CUDA C++ wystarczyło dodanie
odpowiedniej dyrektywy \textit{\_\_device\_\_} przed każdą deklaracją funkcji, która informuje kompilator, że dana funkcja
będzie wykonywana na GPU.
\par
Prymitywy matematyczne dostarczane przez bibliotekę \textit{tiny-bignum-c} nie są jednak wystarczające do przeprowadzenia
operacji na ciele $F_{p}$.
W związku z tym, zostały zaimplementowane dodatkowe operacje modularne takie jak mnożenie, dodawanie, odejmowanie i odwrotność modulo.

\subsubsection{Dodawanie i odejmowanie modulo}
Dodawanie oraz odejmowanie modulo $p$, wygląda bardzo podobnie do standardowego dodawania i odejmowania.
\par
W przypadku odejmowanie dwóch liczb na ciele $F_{p}$, nie ma potrzeby redukcji modulo $p$ po każdej operacji.
Jednak niezbędne jest upewnienie się, że wynik nie jest ujemny. Ponieważ prowadzone obliczenia są na liczbach bez znaku,
to w sytuacji gdy $a-b=c; a<b$, nastąpi przepełnienie i wynik będzie w postaci $2^{32 \cdot n}-1 - c$, gdzie $n$ to rozmiar
wektora do przechowywania liczb. Można bardzo łatwo wrócić do poprawnego wyniku wykorzystując jedną operację
dodawania (wydruk \ref{code:reduction}) i korzystając z faktu, że wszystkie podstawowe operacje są wykonywane modulo $2^{32 \cdot n}$:
$$
    2^{32 \cdot n} - 1 - c + p \equiv p - c \pmod{2^{32 \cdot n}}
$$
\begin{lstlisting}[language=C++, caption={Odejmowanie modulo}, label=code:reduction]
bignum_sub(a, b, c);
if (bignum_cmp(a, b) == SMALLER)
{
    bignum_add(c, p, temp);
    bignum_assign(c, temp);
}
\end{lstlisting}
\par
Dodawanie modulo $p$ jest prostsze.
Aby wykonać dodawanie modularne, wystarczy wykonać standardowe dodawanie i ewentualnie jeżeli $a + b >= p$ zredukować wynik odejmując
od niego liczbę p.

\subsubsection{Mnożenie modulo}
Mnożenie modularne jest bardziej kosztowne niż dodawanie czy odejmowanie, ponieważ wymaga dzielenia z resztą.
W pracy przeprowadzane jest standardowe mnożenie modularne, jednak istnieją bardziej wydajne sposoby,
na przykład wykorzystanie redukcji Barret'a \cite{Menezes2001}.
\par
Do przeprowadzenia klasycznego mnożenia modularnego na ciele $F_{p}$, należy na początku przeprowadzić standardowe mnożenie długich liczb.
Następnie, otrzymany wynik jest dzielony przez $p$ i zwracana jest reszta z dzielenia.
Zarówno mnożenie jak i dzielenie z resztą, są funkcjami dostarczonymi
w bibliotece \textit{tiny-bignum-c}, więc mnożenie modularne sprowadza się do wykonania tych dwóch operacji jedna po drugiej.

\subsubsection{Odwrotność modulo}
Obliczanie odwrotności modulo $p$ jest najbardziej kosztowną operacją na ciele $F_{p}$, głównie ze względu
na wielokrotne dzielenie w pętli.
Algorytm obliczania odwrotności modulo $p$ został zaimplementowany z wykorzystaniem rozszerzonego algorytmu Euklidesa,
który został zmodyfikowany do działania na liczbach nieujemnych. Główna różnica względem klasycznego algorytmu,
polega na wykorzystaniu dodatkowych zmiennych do śledzenia zmian znaku wyniku.
\par
Prototypy wszystkich funkcji do operacji
na długich liczbach, są widoczne na wydruku \ref{code:all-protos} 

\begin{algorithm}[]
    \caption{Odwrotność modularna a mod b}
    \label{alg:inverse}
    \begin{algorithmic}[1]
        \State \text{\textbf{Input: }} $a, b$
        \State $b_0 \gets b$
        \State $x_0 \gets 0$
        \State $x_1 \gets 1$
        \State $x_{0\_sign} \gets 0$
        \State $x_{1\_sign} \gets 0$

        \While{$a > 1$}
        \State $q \gets a \div b$
        \State $t \gets b$
        \State $b \gets a \mod b$
        \State $a \gets t$

        \State $t_2 \gets x_0$
        \State $t_{2\_sign} \gets x_{0\_sign}$
        \State $qx_0 \gets q \times x_0$

        \If{$x_{0\_sign} \neq x_{1\_sign}$}
        \State $x_0 \gets x_1 + qx_0$
        \State $x_{0\_sign} \gets x_{1\_sign}$
        \Else
        \If{$x_1 > qx_0$}
        \State $x_0 \gets x_1 - qx_0$
        \State $x_{0\_sign} \gets x_{1\_sign}$
        \Else
        \State $x_0 \gets qx_0 - x_1$
        \State $x_{0\_sign} \gets 1 - x_{0\_sign}$
        \EndIf
        \EndIf

        \State $x_1 \gets t_2$
        \State $x_{1\_sign} \gets t_{2\_sign}$
        \EndWhile
        \If{$x_{1\_sign} == 1$}
        \State \Return $b - x_1$
        \Else
        \State \Return $x_1$
        \EndIf

    \end{algorithmic}
\end{algorithm}

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=C, caption=Prototypy funkcji wykorzystywanych do operacji na długich liczbach, label=code:all-protos]
__device__ void bignum_init(struct bn *n);
__device__ void bignum_from_int(struct bn *n, DTYPE_TMP i);
__device__ int bignum_to_int(struct bn *n);

__device__ void bignum_add(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_sub(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_mul(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_div(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_mod(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_divmod(
    struct bn *a, struct bn *b, struct bn *c, struct bn *d
    );

__device__ void bignum_assign_fsmall(
    struct bn *dst, struct small_bn *src
    );
__device__ void bignum_assign_small(
    struct small_bn *dst, struct small_bn *src
    );

__device__ void bignum_modinv(struct bn *a, struct bn *b, struct bn *c);

__device__ void bignum_and(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_or(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_xor(struct bn *a, struct bn *b, struct bn *c);
__device__ void bignum_lshift(struct bn *a, struct bn *b, int nbits);
__device__ void bignum_rshift(struct bn *a, struct bn *b, int nbits);

__device__ int bignum_cmp(struct bn *a, struct bn *b);
__device__ int bignum_is_zero(struct bn *n);
__device__ void bignum_inc(struct bn *n);
__device__ void bignum_dec(struct bn *n);
__device__ void bignum_assign(struct bn *dst, struct bn *src);
\end{lstlisting}
\end{minipage}

\subsubsection{Biblioteka CGBN}
W pierwotnej wersji pracy, do operacji na dużych liczbach, była wykorzystywana specjalna biblioteka CGBN dla platformy CUDA. Oferuje ona
wszystkie podstawowe operacje na długich liczbach, takie jak dodawanie, odejmowanie czy mnożenie nawet do 32 tyś. bitów.
Dodatkowo, posiada ona implementację bardziej zaawansowanych funkcji, takich jak odwrotność modulo czy redukcja Barret'a.
Niestety, biblioteka ta narzuca spore ograniczenia pod kątem zasobów. Niezbędne jest grupowanie wątków w grupy
4, 8, 16 lub 32. Wysoką wydajność obliczeń, udało się uzyskać dopiero w grupach składających się z 32 wątków.
Pomimo znacznie szybszego wykonywania się poszczególnych operacji w ramach takiej grupy wątków, narzucone
ograniczenia oraz wysokie użycie rejestrów uniemożliwiało
efektywne zaimplementowanie dużej ilości takich instancji działających równolegle.
Całkowita wydajność mierzona w ilości operacji na krzywej na sekundę osiągnięta z jej wykorzystaniem,
była średnio 4.57 razy gorsza niż
z wykorzystaniem znacznie prostszej implementacji na bazie \textit{tiny-bignum-c}.

\subsection{Funkcja iterująca}
Główna funkcja realizująca kolejne kroki algorytmu Rho Pollard'a została
zaimplementowana jako kernel CUDA.
\subsubsection{Rozpoczęcie obliczeń}
Na początku swojego działania kernel ładuje
punkty wstępnie obliczone do pamięci współdzielonej (\textit{shared memory}),
która pełni funkcję pamięci podręcznej (wydruk \ref{code:flag-init}).

Pierwszy wątek każdego bloku inicjalizuje specjalną flagę \texttt{warp\_finished},
ustawiając jej wartość na \texttt{0}. Flaga ta służy do kontrolowania zakończenia
pracy wszystkich wątków w ramach danego bloku. Po zakończeniu obliczeń przez
przynajmniej jeden wątek, flaga sygnalizuje konieczność zakończenia działania
pozostałych wątków w bloku. Szczegółowe omówienie mechanizmu działania tej flagi
znajduje się w sekcji poświęconej (\textit{tail effect}).

\begin{lstlisting}[language=C, caption=Inicjalizacja pamięci współdzielonej i flagi \texttt{warp\_finished}, label=code:flag-init]
if (threadIdx.x == 0)
{
    printf("STREAM %d BLOCK %d started\n", stream, blockIdx.x);
    for (int i = 0; i < PRECOMPUTED_POINTS; i++)
    {
        bignum_assign_small(&SMEMprecomputed[i].x, &args.precomputed[i].x);
        bignum_assign_small(&SMEMprecomputed[i].y, &args.precomputed[i].y);
    }
    warp_finished = 0;
}
__syncthreads();
\end{lstlisting}

\subsubsection{Wstępnie obliczone punkty}
Dostęp do wstępnie obliczonych punktów jest niezbędny w każdej iteracji \textit{Addition walk}.,
dlatego ważne jest, aby przechowywać je w szybkiej pamięci. Wykorzystałem w tym celu pamięć współdzieloną \textit{shared memory}.
W przeciwieństwie do znacznie większej pamięci globalnej, jest ona przechowywana bezpośrednio na SM \cite{Cheng2014,Jason2011}.
Ponieważ liczba punktów która zapewnia dostatecznie losowy spacer po krzywej eliptycznej
jest stosunkowo niewielka \cite{Teske2000},
to rozmiar pamięci jest wystarczający. W docelowej wersji, przechowywane jest 128 punktów.
\par
Funkcja przydzielająca punkt wstępnie obliczony, na podstawie aktualnie sumowanego punktu została zaimplementowana
poprzez operację AND maski bitowej z pierwszymi 64 bitami współrzędnej
$x$ punktu.
W ten sposób, każdy punkt $W_i$ jest przyporządkowany do jednego z 128 wstępnie obliczonych punktów,
a następnie punkty są dodawane do siebie.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=C, caption=Funkcja przydzielająca punkt]
#define PRECOMPUTED_POINTS 128
__device__ uint32_t map_to_index(bn *x) {
    return (x->array[0] & (PRECOMPUTED_POINTS - 1));
}
\end{lstlisting}
\end{minipage}


\subsubsection{Punkty wyróżnione}
W ramach każdej iteracji, należy w wydajny i szybki sposób sprawdzić, czy obliczony punkt jest punktem wyróżnionym.
W tej implementacji, kryterium warunkującym jest liczba zer na końcu współrzędnej $x$ obliczonego punktu.
\par
Do sprawdzenia, czy punkt jest wyróżniony, słuzy prosta funkcja obliczająca bitową operację AND ze
współrzędnej punktu oraz specjalnej maski bitowej wyznaczanej na podstawie poszukiwanej ilości zer.
Przykładowo, dla poszukiwanej liczby zer 3, maska będzie w postaci $0 ... 00111$. Jeżeli
również ostatnie 3 bity współrzędnej $x$ będzie miało zerowy znak, to otrzymany wynik operacji AND wyniesie 0.
\par
Istotne jest, aby taka sama funkcja została zaimplementowana po stronie serwera na CPU, ponieważ w przypadku znalezienia kolizji,
musi on być w stanie odtworzyć cały spacer losowy prowadzący do danego punktu wyróżnionego.

\begin{algorithm}
    \caption{Funkcja \texttt{is\_distinguish}}
    \begin{algorithmic}[1]
        \State \textbf{Input:} $x$, $liczba\_zer$
        \State $mask \gets 1 \ll liczba\_zer - 1$

        \If{$(x \ \& \ mask) == 0$}
        \State \Return \textbf{true}
        \Else
        \State \Return \textbf{false}
        \EndIf
    \end{algorithmic}
\end{algorithm}

\par
Aby ułatwić testy na różnych etapach implementacji całego systemu,
liczba sprawdzanej ilości zer jest sparametryzowana. Przy starcie każdej serii obliczeń, poszukiwana liczba zer jest jednym z parametrów przekazywanym
do funkcji kernel'a.
\par
Po znalezieniu punktu wyróżnionego, ustawiana jest specjalna flaga w strukturze przechowującej dane punktu oraz
jest on zapisywany pod pierwsze wolne miejsce w pamięci globalnej.

\subsubsection{Obliczanie odwrotnosci w seriach}
Każda operacja dodawania punktów na krzywej eliptycznej we współrzędnych afinicznych, składa się z 3 mnożeń modularnych (M) oraz jednej operacji obliczania odwrotności w ciele (O).
Przeprowadza się również operację dodawania i odejmowania modularnego, jednak ich koszt obliczeniowy jest pomijalnie mały \cite{Blake2005}.
\par
Koszt dodawania punktów na krzywej:
$$
    1O + 3M
$$
Działaniem, które wpływa na wysoki koszt obliczeniowy mnożenia modularnego oraz obliczania odwrotności w ciele,
jest operacja dzielenia \cite{Menezes2001}.
Przykładowo, wykorzystywana w pracy implementacja dzielenia stosuje prosty algorytm \textit{long division},
o złożoności obliczeniowej $O(n^2)$.
\par
Warto zauważyć, że operacja obliczania odwrotności modularnej z wykorzystaniem algorytmu euklidesa, wykonuje dzielenie
podczas obliczania modulo, na każdym etapie pętli (algorytm \ref{alg:inverse}).
To czyni ją najkosztowniejszą obliczeniowo operacją na ciele, znacznie kosztowniejszą niż operacja mnożenia modularnego.
\par
Aby przyśpieszyć obliczenia, w pracy zastosowano technikę obliczania wielu odwrotności równocześnie,
znaną jako \textit{Montgomery Trick} \cite{Montgomery1987}.
\par
Niech $x_1, \cdots ,x_n$ będą elementami, dla których należy policzyć odwrotność.
Na początku obliczana jest tablica elementów, w postaci $a_1 = x_1, a_2 = x_1\cdot x_2 , \cdots, a_n = x_1 \cdots x_n$.
Następnie, obliczana jest odwrotność ostatniego elementu $a_n$ za pomocą jednej operacji odwrotności w ciele.
Teraz, aby policzyć odwrotność elementu $x_n$ wystarczy wykonać jedynie operację mnożenia
$b_n = a_{n-1} \cdot a_{n}^{-1}$. Kolejne elementy są obliczane analogicznie, za pomocą mnożenia:
$b_{n-1} = a_{n-2} \cdot b_{n}$.
\textit{Montgomery trick} pozwala na zamianę:
$$
    nO = O + 3(n - 1)M
$$
\par
Sposób implementacji tej tej metody wymagał podjęcia paru decyzji.
Pierwszym problemem który pojawia się w metodzie Montgomery'ego,
jest konieczność
obliczania iteracji dla wielu punktów jednocześnie.
\par
Jednym ze sposobów by tego dokonać,
jest zsynchronizowanie wielu wątków w ramach bloku obliczeniowego, a następnie przekazanie
jednemu z nich, za pomocą pamięci współdzielonej, wszystkich liczb do obliczenia odwrotności.
Następnie, wątek zwracałby obliczone odwrotności za pomocą pamięci współdzielonej.
Jak zauważono w pracy \cite{Boss2015}, takie podejście nie jest optymalne w przypadku GPU.
Wymagałoby to sporo synchronizacji pomiędzy wątkami, oraz sporej ilości zapisów i odczytów
z pamięci współdzielonej, która pomimo bycia znacznie szybszą niż pamięć globalna, nie jest tak
szybka jak prywatna pamięć w postaci rejestrów. Dodatkowo, z racji, że tylko jeden
wątek oblicza odwrotności, pozostałe muszą bezczynnie czekać.
\par
Dlatego sposobem, który został zastosowany jest obliczanie wielu odwrotności
w ramach jednego wątku. Oznacza to, że każdy wątek zamiast przetwarzać tylko jeden punkt startowy,
dostaje ich $n$ na początku działania programu.
\par
\textit{Naiwne} podejście polegałoby na przekazaniu każdemu z wątków $n$ punktów startowych,
i oczekiwanie aż znajdzie dla każdego punktu startowego odpowiadający mu punkt wyróżniony. Taki sposób powoduje,
że bardzo szybko zysk z metody Montgomery'ego jest tracony, a nawet zaczyna pojawiać się dodatkowy koszt obliczeniowy,
z powodu obliczeń, których nie wykorzystujemy.
Wynika to z faktu,
że po znalezieniu $i$ punktów, zysk z metody jest postaci:
$$
    (n-i)O = O + 3(n-i)M + 3(i)M
$$
Gdzie $3(i)M$ to mnożenia, które nie przyczyniają się do znalezienia nowych punktów wyróżnionych, więc należy je traktować jako niepotrzebne spowolnienie.
\par
W celu rozwiązania tego problemu, w pracy zostało zastosowanie okienkowanie obliczeń w tablicy o stałym rozmiarze $m$.
Sposób ten wymaga, aby każdy wątek otrzymał $n > m$ punktów startowych w wyznaczonym dla niego miejscu w pamięci globalnej.
Im większa różnica $n-m$ tym lepszy
stosunek czasu obliczeń do narzutu czasowego związanego ze startem programu.
\par
Na początku działania wątku, ładowane jest do tablicy okna kolejne $m$ punktów startowych z pamięci globalnej.
Następnie, w pętli obliczane są odwrotności wymagane dla operacji dodawania punktów i przeprowadzane
jest ich dodawanie w ramach algorytmu Rho Pollard'a.
Tym sposobem, w jednym kroku pętli, wykonywana jest jedna iterację algorytmu Rho Pollard'a dla $m$ punktów.
Na samym końcu każdego kroku pętli,
jest sprawdzane czy któryś z obliczonych punktów jest punktem wyróżnionym. Jeżeli tak,
znaleziony punkt zapisywany jest na pierwszym wolnym miejscu w pamięci globalnej, a na jego miejsce do tablicy
okna ładowany jest kolejny punkt startowy.
Dzięki temu, przez większość czasu obliczeń, wykorzystywany jest pełny zysk z metody Montgomery'ego.
Przykład dla $m = 3$ ilustrujący działanie okna, znajduję się na rys. \ref{fig:windowing}.
Punkty startowe zostały przedstawione jako \textit{PS}, znaleziony punkt wyróżniony jako \textit{PW}.
\par
Jeżeli dodatkowo zastosujemy flagę, która kończy obliczenia wszystkich wątków w bloku, gdy pierwszy wątek,
znajdzie $n-m$ punktów wyróżnionych, to otrzymujemy maksymalny poziom wydajności w ciągu całej fali obliczeń.
Efekt ten jest dokładniej opisany w części poświęconej problemowi \textit{tail effect}.

% schemat
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/okienkowanie.png}
    \caption{Schemat działania jednego kroku pętli z wykorzystaniem okna}
    \label{fig:windowing}
\end{figure}

\subsection{Tail effect}
Tail Effect to zjawisko, które polega na nierównomiernym obciążeniu wątków na GPU.
W przypadku równoległej wersji algorytmu Rho Pollard'a, to zjawisko jest szczególnie widoczne,
ze względu na losowość funkcji błądzenia po krzywej.
Wątki znajdują punkty wyróżnione w różnym, losowym czasie.
\par
Po znalezieniu punktu wyróżnionego, wątek musi zaczekać, na zakończenie pracy wszystkich
pozostałych wątków w ramach jednego bloku.
To powoduje, że tylko przez krótki czas są wykorzystywane wszystkie zasoby GPU.
\par
Jednym ze sposobów na zmniejszenie tego efektu, jest wydłużenie czasu pracy
każdego z wątków poprzez zwiększenie liczby punktów wyróżnionych,
które musi znaleźć. Dzięki temu spada prawdopodobieństwo,
że zakończy on swoją pracę znacznie wcześniej niż pozostałe wątki w bloku.
Zastosowanie metody z oknem, opisanej w poprzedniej sekcji, pozwala na znacznie lepszą utylizację zasobów przez większość
czasu obliczeń.
\par
Rys \ref{fig:tail_effect_1} oraz \ref{fig:tail_effect_3} przedstawiają czas pracy każdego z wątków w bloku,
w trakcie jednej fali obliczeń.
W trakcie testu, każdy z wątków poszukiwał punktów wyróżnionych
z 17 zerami na końcu współrzędnej $x$. Widoczne jest
znaczne lepsze wykorzystanie zasobów GPU w przypadku, gdy każdy z wątków musi znaleźć
3 punkty wyróżnione przed zakończeniem pracy (rys. \ref{fig:tail_effect_3}).
Zmniejsza to wpływ anomalii, gdy punkt wyróżniony jest znajdowany znacznie wcześniej
lub później niż wynika z wartości oczekiwanej.

% wykres
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/tailing_effect_single_17.png}
    \caption{Tail effect. Wątki kończą pracę po znalezieniu jednego punktu wyróżnionego.}
    \label{fig:tail_effect_1}
\end{figure}

% wykres
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/tailing_effect_3_17.png}
    \caption{Tail effect. Wątki kończą pracę po znalezieniu 3 punktów wyróżnionych.}
    \label{fig:tail_effect_3}
\end{figure}

\par
Aby jeszcze bardziej zniwelować efekt, została zastosowana specjalna flaga w pamięci współdzielonej,
która pozwala zakończyć obliczenia wszystkich wątków w bloku,
po tym, jak pierwszy z nich znajdzie wszystkie punkty wyróżnione. Pozwala to zwolnić miejsce na kolejny blok
i pozwolić na załadowanie nowych danych do obliczeń, gdy tylko poziom wykorzystania zasobów GPU zacznie spadać.
Karty graficzne z architekturą Turing,
pozwalają na kolejkowanie wielu uruchomień kernel'a wykorzystując mechanizm strumieni CUDA.
Dzięki wykorzystaniu strumieni, scheduler CUDA
może uruchamiać kolejne bloki obliczeń, gdy tylko
zwolni się miejsce na kolejny blok, zamiast czekać ze startem na zakończenie poprzedniego kernel'a \cite{Jason2011}.
\par
Rys \ref{fig:streams} przedstawia wykres sumy znalezionych punków od czasu przy uruchomieniu dwóch
kernel'i w oddzielnych strumieniach.
Każdy z wątków szukał 10 punktów wyróżnionych z 22 zerami na końcu współrzędnej $x$.
Zastosowano flagę kończenia obliczeń wszystkich wątków w bloku.
\par
Widoczne jest przejmowanie przez strumień Stream 2 wolnych zasobów, zwalnianych przez Stream 1.
Wypłaszczenie krzywej sumy punktów pod koniec pracy każdego ze strumieni, jest
efektem ogona, spowodowanym przez nierównomierny czas pracy na poziomie bloków.
Jednak dopóki jest zapewniona zastępowalność bloków na SM, efekt nie wpływa na wydajność obliczeń.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{img/streams.png}
    \caption{Suma znalezionych punktów wyróżnionych z podziałem na strumienie.}
    \label{fig:streams}
\end{figure}

\subsection{Program serwera}
Główny cel serwera centralnego w równoległej wersji algorytmu Rho Pollard'a, sprowadza się do
przechowywania obliczonych punktów wyróżnionych oraz poszukiwania wśród nich kolizji.
W tej pracy, jego zadania zostały rozszerzone o generowanie punktów startowych
oraz wstępną generację punktów niezbędnych do spaceru losowego w wersji \textit{addition walk}.
\par
Na samym początku działania systemu, serwer startuje odpowiednią ilość
klientów, poprzez uruchamianie w osobnych wątkach funkcji \textit{GPUWorker},
w ilości na tyle dużej, aby wysycić zasoby GPU.
Ponieważ kod działający na GPU został skompilowany z flagą dla kompilatora NVCC \textit{--default-stream per-thread},
każdy z klientów działających w osobnym wątku, tworzy własny strumień CUDA, co pozwala na kolejkowanie
uruchomionych kernel'i po stronie GPU.
\par
Klienci komunikuje się z serwerem za pomocą dwóch asynchronicznych kolejek FIFO.
Pierwsza z nich służy do przekazywania punktów startowych z serwera do klientów, a druga do przekazywania
znalezionych punktów wyróżnionych przez klientów do serwera. Taka centralizacja
generowania punktów startowych, wynika z problemów z bibliotekami SageMath,
podczas uruchamiania ich kodu w wielu wątkach jednocześnie.
Serwer w celu przechowywania punktów otrzymanych od klientów, wykorzystuje
zwykły słownik dostępny w języku Python, który jest odpowiednikiem hash-mapy.
Punkty są przechowywane w formie: \textit{(x,y): seed}, gdzie \textit{seed} oznacza
ziarno z jakiego został wygenerowany punkt startowy, który doprowadził do znalezienia punktu wyróżnionego.
Dzięki temu, w razie kolizji, serwer jest w stanie odtworzyć punkt startowy,
a następnie wykonać cały spacer losowy, który doprowadził do danego punktu.
Jest to szczególnie istotne, ponieważ po stronie GPU nie są zliczane parametry $a$ oraz $b$, niezbędne do obliczenia
logarytmu dyskretnego

\subsubsection{Generacja punktów startowych}
Początkowo, punkty startowe były generowane za pomocą funkcji skrótu MD5 z operacją
modulo rzędu ciała, jednak bardziej wydajnym podejściem okazało się wykorzystanie
pakietu \texttt{random} z biblioteki standardowej języka Python. Pakiet ten
zapewnia wystarczającą losowość do tego zastosowania, co umożliwia szybszą
i efektywną generację punktów.

Proces generacji punktów startowych odbywa się wyłącznie po stronie serwera (wydruk \ref{code:point-gen}).
Wygenerowane punkty, wraz z odpowiadającymi im ziarnami (\textit{seed}), są
przekazywane do klienta. Jeśli jednak wygenerowany punkt startowy spełnia od razu
kryteria punktu wyróżnionego, zostaje przekazany bezpośrednio do puli punktów
wyróżnionych i nie zajmuje miejsca wśród punktów startowych (linia 14). Dzięki temu zasoby
są lepiej wykorzystywane, a redundancja w obliczeniach jest zredukowana.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Python, caption=Generacja punktów startowych, label=code:point-gen]
def generate_starting_points(instances, zeros_count):
    distinguish_points = []
    starting_points = []
    i = 0
    while i < instances:
        seed = int.from_bytes(random.randbytes(10), "big") % curve_order
        A = P * seed
        x = int(A[0])
        y = int(A[1])
        if not is_distinguish(x, zeros_count):
            i += 1
            starting_points.append((x, y, seed))
        else:
            distinguish_points.append((x, y, seed))
    return starting_points, distinguish_points
\end{lstlisting}
\end{minipage}

\subsubsection{Kolizje}

W przypadku wykrycia kolizji serwer pobiera ziarna obu punktów i odtwarza
kolejne kroki algorytmu, które doprowadziły do ich znalezienia. W tym procesie
serwer oblicza wielokrotności punktów $P$ oraz $Q$, co pozwala na określenie
logarytmu dyskretnego. W rzadkich przypadkach, gdy obliczenie odwrotności modularnej
jest niemożliwe (np. mianownik wynosi $0$), obliczenie
logarytmu dyskretnego może się nie powieść.

Jeżeli jednak obliczenia zakończą się sukcesem, serwer wyznacza logarytm
dyskretny i wysyła sygnał do zakończenia dalszych obliczeń. Wynik końcowy jest
następnie przekazywany na standardowe wyjście (\texttt{stdout}).

Implementacja funkcji obliczającej współczynniki $a$ i $b$ w Pythonie:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Python, caption=Obliczanie współczynników $a$ i $b$]
def calculate_ab(seed, precomputed_points: list[PrecomputedPoint]):
    a_sum = seed
    b_sum = 0
    W = P * seed
    while not is_distinguish(W[0], ZEROS_COUNT):
        precomp_index = map_to_index(W[0])
        precomputed = precomputed_points[precomp_index]
        R = precomputed.point
        a_sum = a_sum + precomputed.a
        b_sum = b_sum + precomputed.b
        W = W + R
    a_sum = a_sum % curve_order
    b_sum = b_sum % curve_order
    return (a_sum, b_sum)
\end{lstlisting}
\end{minipage}

Funkcja \texttt{calculate\_ab} przyjmuje ziarno oraz listę wcześniej
obliczonych punktów. Kolejne wielokrotności punktów $P$ i $Q$ są sumowane wraz
z ich współczynnikami $a$ i $b$, aż do napotkania punktu wyróżnionego. Wynikowe
wartości są redukowane modulo rząd krzywej i zwracane jako współczynniki, które
umożliwiają wyznaczenie logarytmu dyskretnego.

\subsection{Logowanie}

W celu monitorowania wydajności systemu oraz postępu obliczeń, zarówno w kodzie
klienta, jak i serwera, zaimplementowano mechanizm logowania. CUDA umożliwia
wypisywanie informacji na standardowe wyjście (\texttt{stdout}) za pomocą funkcji
\texttt{printf}, analogicznie jak w standardowym języku C. W kodzie głównego kernel'a
można aktywować logowanie, ustawiając odpowiednią flagę za pomocą dyrektywy
\texttt{\#define logging}.

Logowane dane w kernel'u obejmują:
\begin{itemize}
    \item czas znalezienia punktu wyróżnionego,
    \item numer wątku,
    \item numer bloku,
    \item numer strumienia.
\end{itemize}

Przykładowy fragment logów generowanych przez kernel:
\begin{verbatim}
Wed Sep 18 01:14:33 AM CEST 2024 STREAM 0, instance: 8253 found distinguish point 0
Wed Sep 18 01:14:33 AM CEST 2024 STREAM 0, instance: 28320 found distinguish point 0
Wed Sep 18 01:14:33 AM CEST 2024 STREAM 0, instance: 19113 found distinguish point 0
Wed Sep 18 01:14:33 AM CEST 2024 STREAM 0, instance: 28214 found distinguish point 0
Wed Sep 18 01:14:33 AM CEST 2024 STREAM 0, instance: 24343 found distinguish point 0
\end{verbatim}

Dodatkowo, serwer po każdorazowym otrzymaniu danych od jednego z wątków klienta,
wypisuje informacje o łącznej liczbie punktów wyróżnionych które
zgromadził do tej pory. Przykładowe logi generowane przez serwer:
\begin{verbatim}
Wed Sep 18 07:13:45 PM CEST 2024 Got new distinguish points
Wed Sep 18 07:13:45 PM CEST 2024 Currently have 941239
Wed Sep 18 07:13:45 PM CEST 2024 GPU worker 0 got task
Wed Sep 18 07:13:45 PM CEST 2024 Got 957733 points
\end{verbatim}

Mechanizm logowania umożliwił dokładniejszą analizę wykorzystania zasobów GPU oraz
optymalne dostosowanie liczby wątków i bloków do struktury sprzętowej. Dane logowane
były zapisywane do pliku, co pozwoliło na dalsze przetwarzanie, takie jak mierzenie
wydajności i wizualizację wyników w formie wykresów. Wizualizacje te przedstawiają
między innymi sumę znalezionych punktów od czasu,
co pozwoliło oszacować wydajność projektu.
